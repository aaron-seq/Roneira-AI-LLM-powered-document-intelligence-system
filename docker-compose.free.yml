# ==============================================================================
# Free & Open Source Docker Compose Configuration
# Optimized for deployment on free hosting platforms like Railway, Render, etc.
# Using Ollama with DeepSeek models for better performance
# ==============================================================================

version: '3.8'

services:
  # ------------------------------------------------------------------------------
  # Application Service (FastAPI Backend)
  # ------------------------------------------------------------------------------
  app:
    build:
      context: .
      dockerfile: Dockerfile
      target: ${DOCKER_BUILD_TARGET:-production}
    container_name: roneira-ai-app
    restart: unless-stopped
    ports:
      - "${PORT:-8000}:8000"
    environment:
      - ENVIRONMENT=${ENVIRONMENT:-production}
      - DATABASE_URL=${DATABASE_URL:-sqlite+aiosqlite:///./roneira.db}
      - REDIS_URL=${REDIS_URL:-redis://redis:6379/0}
      - SECRET_KEY=${SECRET_KEY:-change-this-in-production}
      - OLLAMA_HOST=ollama:11434
      - USE_LOCAL_LLM=true
      - LLM_MODEL_NAME=${LLM_MODEL_NAME:-deepseek-coder:6.7b}
      - DEEPSEEK_MODEL_FALLBACK=deepseek-r1:7b
      - OCR_ENGINE=tesseract
      - UPLOAD_PATH=/app/uploads
      - MAX_FILE_SIZE_MB=50
      - ENABLE_CLOUD_FALLBACK=true
      - DEEPSEEK_API_URL=${DEEPSEEK_API_URL:-https://api.deepseek.com/v1}
    volumes:
      - uploads_data:/app/uploads
      - logs_data:/app/logs
    depends_on:
      - redis
      - ollama
    networks:
      - roneira_network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # ------------------------------------------------------------------------------
  # Ollama Service (Local LLM with DeepSeek Models)
  # ------------------------------------------------------------------------------
  ollama:
    image: ollama/ollama:latest
    container_name: roneira-ollama
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_ORIGINS=*
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_MAX_LOADED_MODELS=2
      - OLLAMA_NUM_PARALLEL=2
    networks:
      - roneira_network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/version"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

  # ------------------------------------------------------------------------------
  # Redis Service (Caching & Queue)
  # ------------------------------------------------------------------------------
  redis:
    image: redis:7-alpine
    container_name: roneira-redis
    restart: unless-stopped
    command: redis-server --maxmemory 128mb --maxmemory-policy allkeys-lru
    volumes:
      - redis_data:/data
    ports:
      - "6379:6379"
    networks:
      - roneira_network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 5

  # ------------------------------------------------------------------------------
  # PostgreSQL Database (Optional - can use SQLite for lighter deployment)
  # ------------------------------------------------------------------------------
  postgres:
    image: postgres:15-alpine
    container_name: roneira-postgres
    restart: unless-stopped
    environment:
      - POSTGRES_DB=${POSTGRES_DB:-roneira}
      - POSTGRES_USER=${POSTGRES_USER:-postgres}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-password}
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    networks:
      - roneira_network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-postgres}"]
      interval: 10s
      timeout: 5s
      retries: 5
    profiles:
      - postgres  # Only start when postgres profile is active

  # ------------------------------------------------------------------------------
  # Nginx (Optional - for serving static files and reverse proxy)
  # ------------------------------------------------------------------------------
  nginx:
    image: nginx:alpine
    container_name: roneira-nginx
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
      - uploads_data:/var/www/uploads:ro
    depends_on:
      - app
    networks:
      - roneira_network
    profiles:
      - nginx  # Only start when nginx profile is active

  # ------------------------------------------------------------------------------
  # DeepSeek Model Initialization Service
  # ------------------------------------------------------------------------------
  model-init:
    image: ollama/ollama:latest
    container_name: roneira-model-init
    depends_on:
      ollama:
        condition: service_healthy
    volumes:
      - ollama_data:/root/.ollama
    networks:
      - roneira_network
    command: >
      sh -c "
        echo 'Pulling DeepSeek models...' &&
        ollama pull deepseek-coder:6.7b &&
        ollama pull deepseek-r1:7b &&
        echo 'DeepSeek models ready!' &&
        echo 'Setting up model aliases...' &&
        ollama copy deepseek-coder:6.7b document-ai &&
        echo 'Model setup complete!'
      "
    restart: "no"
    profiles:
      - init  # Only run when init profile is active

# ==============================================================================
# Networks
# ==============================================================================
networks:
  roneira_network:
    driver: bridge
    name: roneira_network

# ==============================================================================
# Volumes
# ==============================================================================
volumes:
  postgres_data:
    name: roneira_postgres_data
  redis_data:
    name: roneira_redis_data
  ollama_data:
    name: roneira_ollama_data
  uploads_data:
    name: roneira_uploads_data
  logs_data:
    name: roneira_logs_data